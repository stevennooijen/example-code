{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow, Kaggle submission\n",
    "\n",
    "This notebook is meant to demonstrate how one could experiment with different models through MLflow. \n",
    "\n",
    "The goal is to log the different model experiment results MLflow and use the [MLflow UI](https://www.mlflow.org/docs/latest/quickstart.html#viewing-the-tracking-ui) to pick the best model. You should then have a first understanding of how MLflow could help you with model selection.\n",
    "\n",
    "This notebooks uses data from the Kaggle [Amazon Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge), which is a binary classsification problem. You can do a late submission at the end of this notebook to see how your model compares to the other approaches in the leaderboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Preparing the virtualenv\n",
    "\n",
    "First make sure you have installed the packages that we need in your virtualenv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle mlflow python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting MLflow and Kaggle credentials through `python-dotenv`\n",
    "\n",
    "For tracking and storing your model experimentation results, MLflow uses a tracking server. There are two options:\n",
    "1. Run the tracking server locally with command `mlflow ui`. This exposes the UI on http://localhost:5000.\n",
    "2. Set up an tracking server and connect to it by setting the right environment variables.\n",
    "\n",
    "In order for Python to know how to write to the MLflow tracking server, you need to set a couple of environment variables. This notebook uses the [`python-dotenv`](https://pypi.org/project/python-dotenv/) package to load these into this notebook. The package assumes all your environemt variable sare neatly saved in a `.env` file, which is gitignored to prevent us from uploading passwords into Git. \n",
    "\n",
    "We also need credentials to use the Kaggle API for downloading data. These can be added to the `.env` file as well.\n",
    "\n",
    "**Firsts task**: Copy the `.env-example` file and save it as `.env`. Then fill in the passwords and usernames required for MLflow and Kaggle.\n",
    "\n",
    "**Note**: When running the MLflow UI locally you only need to set `MLFLOW_TRACKING_URI='http://127.0.0.1:5000'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init\n",
    "\n",
    "Having prepared your environment variables it's time to load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check where you are going to serve the MLflow UI from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore load packages you need and set a random seed for reproducability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "You can easily download the Kaggle data by calling the Kaggle API. First select a folder you want to store the files in and create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data/amazon-employee-access-challenge/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the Kaggle API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c amazon-employee-access-challenge -p $data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And validate that the files are indeed there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When submitting to Kaggle it's good to know what the required format is for your submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $data_folder/sampleSubmission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in the other data sets for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_folder + 'train.csv')\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `test.csv` to create your submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_folder + 'test.csv')\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Check out the data sets for yourself to see how they look like and what variables you can use for a model.\n",
    "\n",
    "Specifically, look into the class distribution of your target variable. What kind of model would you apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "\n",
    "For this demo we use `sklearn`. It requires your data to be in a specific shape, so let's do that for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.copy()\n",
    "y_train = X_train.pop('ACTION')\n",
    "\n",
    "(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop('id', axis=1)\n",
    "y_id = df_test[['id']].rename(columns={\"id\": \"Id\"})\n",
    "\n",
    "(X_test.shape, y_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting mlflow experiment\n",
    "\n",
    "In MLflow you can define experiments. Familiarize yourself a bit with the [documentation](https://mlflow.org/docs/latest/quickstart.html), and check out what `set_experiment` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own experiment here\n",
    "mlflow.set_experiment(<fill-in>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a single model\n",
    "\n",
    "Let's run a model once without logging to MLflow yet. Choose one of your own and return the metrics that report its quality of fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn dependencies\n",
    "from sklearn.<fill-in> \n",
    "\n",
    "# create a model\n",
    "model = <fill-in>\n",
    "\n",
    "# fit, run, and/or score your model\n",
    "<fill-in>\n",
    "\n",
    "# print the metrics that you are interested in\n",
    "<fill-in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/simple-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Are you ready to tweak the model and use MLflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "The next step is to tune our model, and find out the optimal hyperparameters. In this case, let's also see how we can start logging the results into MLflow!\n",
    "\n",
    "Firstly: define a parameter grid that you want to search over. For example for the `LogisticRegression`, do something like:\n",
    "\n",
    "```\n",
    "C_range = np.logspace(-4.0, 4.0, num=10)\n",
    "param_grid = {'C': C_range,'penalty': ['l1','l2'] }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the param_grid for your model of choice:\n",
    "param_grid = <fill-in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our grid search, we need to loop over all possible combinations within this `param_grid`. Here's some code that creates all combinations for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# create list to iterate hyperparameters over\n",
    "def create_param_grid_list(param_grid):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    params_list = []\n",
    "    for v in product(*values):\n",
    "        params_list.append(dict(zip(keys, v)))\n",
    "    return params_list\n",
    "\n",
    "params_list = create_param_grid_list(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now just loop over all combinations, run our model with those specific hyperparameter settings and write them to MLflow! \n",
    "\n",
    "Within the loop, just use the same logic as you did above when you were running just a single model. However, instead of printing your metrics, now write them to MLflow. \n",
    "\n",
    "**Task**: Check out the documentation on `mlflow.start_run()`. What does it do? \n",
    "\n",
    "**Task**: We are writing both metrics and params to mlflow. What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in params_list:\n",
    "    \n",
    "    # fill in a run_name that you think is convenient\n",
    "    with mlflow.start_run(run_name=<fill-in>):\n",
    "\n",
    "        # create your model with the params of this loop\n",
    "        model = <fill-in>\n",
    "\n",
    "        # fit, run, and/or score your model\n",
    "        <fill-in>\n",
    "\n",
    "        # write the metrics that you are interested in to mlflow\n",
    "        # for example: \n",
    "        mlflow.log_param(<fill-in>)\n",
    "        # for example: \n",
    "        mlflow.log_metric(<fill-in>)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/gridsearch-example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now check out your results in the MLflow UI.\n",
    "\n",
    "**Task:** Compare the different run results and pick the model that you think is best. Did you have enough information for your decisison? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and submit to Kaggle!\n",
    "\n",
    "Great! Now you have your besst model, finish the job by retraining it once on the entire dataset and submitting your reuslts to Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "model = <fill-in>\n",
    "\n",
    "# fit it on the entire dataset\n",
    "<fill-in>\n",
    "\n",
    "# predict on the test set\n",
    "pred = <fill-in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/fit-predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare your predictions and save appropriately for the Kaggle submission format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "pred = pd.Series(pred, name='Action')\n",
    "\n",
    "submission = pd.concat([y_id, pred], axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally write to csv and upload!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(data_folder + 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $data_folder/submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c amazon-employee-access-challenge -f $data_folder/submission.csv -m \"My submission!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.\n",
    "\n",
    "**Task:** What is your score?! \n",
    "\n",
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
